{"cells":[{"cell_type":"markdown","source":[" # Reading and Writing Data with Spark\n","\n"," This notebook contains the code from the previous screencast. The only difference is that instead of reading in a dataset from a remote cluster, the data set is read in from a local file. You can see the file by clicking on the \"jupyter\" icon and opening the folder titled \"data\".\n","\n"," Run the code cell to see how everything works.\n","\n"," First let's import SparkConf and SparkSession"],"metadata":{}},{"source":["import pyspark\n","from pyspark import SparkConf\n","from pyspark.sql import SparkSession\n","import requests, zipfile, io, os\n","import pandas as pd \n","from pyspark import SparkContext, SparkConf\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":1},{"cell_type":"markdown","source":[" Since we're using Spark locally we already have both a sparkcontext and a sparksession running. We can update some of the parameters, such our application's name. Let's just call it \"Our first Python Spark SQL example\""],"metadata":{}},{"source":["spark = SparkSession     .builder     .appName(\"NFIP Dataset\")     .getOrCreate()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":2},{"cell_type":"markdown","source":[" Let's check if the change went through"],"metadata":{}},{"source":["spark.sparkContext.getConf().getAll()\n","\n",""],"cell_type":"code","outputs":[{"output_type":"execute_result","data":{"text/plain":"[('spark.app.id', 'local-1568754611572'),\n ('spark.rdd.compress', 'True'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.driver.port', '58264'),\n ('spark.driver.host', 'LAPTOP-1PQMVB7U.home'),\n ('spark.master', 'local[*]'),\n ('spark.executor.id', 'driver'),\n ('spark.app.name', 'NFIP Dataset'),\n ('spark.submit.deployMode', 'client'),\n ('spark.ui.showConsoleProgress', 'true')]"},"metadata":{},"execution_count":3}],"metadata":{},"execution_count":3},{"source":["spark\n","\n",""],"cell_type":"code","outputs":[{"output_type":"execute_result","data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x231103fc1d0>","text/html":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://LAPTOP-1PQMVB7U.home:4041\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>NFIP Dataset</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{},"execution_count":4}],"metadata":{},"execution_count":4},{"cell_type":"markdown","source":[" Let's download the datasets...this could take sometime"],"metadata":{}},{"source":["\n","all_zip_file_urls = [\n","                     'https://www.fema.gov/media-library-data/1566235780855-42d2142c3b1c2520a205192774268f84/openFEMA_policies20190531_1.zip',\n","                     'https://www.fema.gov/media-library-data/1566235780855-42d2142c3b1c2520a205192774268f84/openFEMA_policies20190531_2.zip',\n","                     'https://www.fema.gov/media-library-data/1566235780855-42d2142c3b1c2520a205192774268f84/openFEMA_policies20190531_3.zip',\n","                     'https://www.fema.gov/media-library-data/1566235431170-4120327dea121daff89b8ec3da22b832/openFEMA_policies20190531_4.zip',\n","                     'https://www.fema.gov/media-library-data/1566235780855-42d2142c3b1c2520a205192774268f84/openFEMA_policies20190531_5.zip'\n","                    ]\n","\n","for zip_file_url in all_zip_file_urls:\n","    r = requests.get(zip_file_url)\n","    z = zipfile.ZipFile(io.BytesIO(r.content))\n","    z.extractall()\n","\n","\n",""],"cell_type":"code","outputs":[{"output_type":"error","ename":"BadZipFile","evalue":"File is not a zip file","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-5-c28d4ab4fcb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mzip_file_url\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_zip_file_urls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip_file_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[0;32m   1220\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1221\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1222\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1223\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m                 \u001b[1;31m# set the modified flag so central directory gets written\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1289\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"]}],"metadata":{},"execution_count":5},{"cell_type":"markdown","source":[" Lets grab all the files in the directory"],"metadata":{}},{"source":["\n","import glob\n","\n","path = 'C:\\\\Users\\\\Jared\\\\OneDrive\\\\Documents\\\\GitHub\\\\nfip_data_prep'\n","files = [f for f in glob.glob(path + \"**/*.csv\", recursive=True)]\n","\n","for f in files:\n","    print(f)\n","\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" And just split them into claims and policy dataframes"],"metadata":{}},{"source":["\n","claims_path = [filename for filename in files if filename.endswith(\"openFEMA_claims20190531.csv\")]\n","df_claims = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"mode\",\"DROPMALFORMED\").load(claims_path[0])\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_claims.printSchema() \n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","policies_path = [filename for filename in files if not filename.endswith(\"openFEMA_claims20190531.csv\")]\n","df_policies_1 = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"mode\",\"DROPMALFORMED\").load(policies_path[0])\n","\n","\n","for policies in policies_path[1:]:\n","    df_policies = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"mode\",\"DROPMALFORMED\").load(policies) \n","    df_policies_1.union(df_policies)\n","\n","\n","df_policies_1.printSchema() \n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_claims.describe().show()\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["df_policies_1.describe().show()\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Now lets cut the policy and claims dataset down to just NY\n"," then lets write to disk"],"metadata":{}},{"source":["df_claims_out = df_claims.filter(df_claims.state == \"NY\").collect()\n","df_policies_1_out = df_policies_1.filter(df_policies_1.state == \"NY\").collect(\n","\n","claims_out_path = \"claims.csv\"\n","policy_out_path = \"policies.csv\"\n","\n","df_claims_out.write.save(claims_out_path, format=\"csv\", header=True)\n","df_policies_1_out.write.save(policy_out_path, format=\"csv\", header=True)\n","\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}